{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c350fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id: tensor([[2769]])\n",
      "outputs: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.2313, -0.3789,  0.0102,  ..., -0.9358,  0.0685,  0.3635]]]), pooler_output=tensor([[ 0.9827,  0.2063,  0.6847,  ..., -0.9192,  0.0569, -0.8858]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "embeddings: tensor([ 4.7020e-01,  2.6461e-01, -2.1432e-01, -5.2927e-02,  3.0778e-01,\n",
      "        -2.3462e-02,  2.2461e-01, -1.9313e-01,  3.3127e-02, -3.2388e-01,\n",
      "        -1.1657e-02, -6.6924e-02, -5.0658e-02,  3.2662e-01, -3.0206e-01,\n",
      "        -1.0265e-01, -9.0821e-02,  3.2740e-01,  9.3796e-02,  1.9723e-01,\n",
      "         1.1347e-02, -1.7763e-01, -8.4163e-03, -6.0926e-02, -7.6311e-03,\n",
      "         3.2154e-01, -5.1560e-02,  6.0643e-02, -1.9824e-01, -2.5021e-02,\n",
      "        -1.6107e-02, -4.2907e-01,  4.1345e-02,  3.6247e-01,  2.1230e-01,\n",
      "        -1.1116e-01,  2.4410e-01, -2.7897e-01, -1.9281e-01, -1.2328e-01,\n",
      "         1.3469e-01,  8.1929e-02,  2.0420e-01, -6.4993e-02,  1.3753e-01,\n",
      "         5.9630e-02, -2.3586e-01, -3.4202e-01, -5.6271e-01,  4.0041e-02,\n",
      "        -1.5880e-01,  5.0069e-02, -5.4962e-02,  2.0367e-01,  3.0205e-01,\n",
      "         2.3925e-01,  1.6342e-01,  2.2661e-01,  3.0389e-02,  2.0420e-01,\n",
      "         2.7638e-01,  9.0203e-02, -2.6882e-01, -1.6655e-02,  3.0207e-01,\n",
      "         1.1740e-02,  2.9319e-01, -1.1938e-01, -2.7657e-01,  1.8941e-01,\n",
      "        -4.1036e-02, -3.9574e-01,  4.8843e-01, -6.9752e-02,  7.6346e-02,\n",
      "         1.0742e-01, -5.3716e-02,  1.2043e-01, -2.7951e-02, -2.1025e-01,\n",
      "         3.3702e-01,  2.6611e-01,  1.1711e-01,  2.2948e-01,  8.3858e-02,\n",
      "         1.0039e-02, -2.6238e-01, -9.0332e-02, -3.1028e-01,  1.6366e-01,\n",
      "        -2.4171e-01,  4.9576e-02, -7.1779e-02, -7.2060e-02,  1.9803e-02,\n",
      "        -6.1183e-03, -2.1082e-01, -1.1660e-01,  4.8472e-01, -1.7354e-01,\n",
      "        -1.6205e-02,  1.1028e-01, -3.7704e-02,  3.0511e-01, -1.9639e-01,\n",
      "         8.3272e-02, -1.5112e-01, -6.2073e-02,  2.9999e-02, -9.0039e-01,\n",
      "         2.5298e-01, -7.4246e-02, -2.1034e-01, -2.3996e-01,  5.8296e-02,\n",
      "         4.5886e-01,  2.8176e-01, -1.1755e-01, -1.7096e-01,  4.2738e-01,\n",
      "        -4.8113e-01, -8.4365e-02, -1.2417e-01,  6.1477e-01, -1.6622e-01,\n",
      "         2.2148e-01, -7.3975e-02, -8.7307e-02,  1.5254e-01, -3.0966e-01,\n",
      "        -1.5482e-01,  4.4547e-01, -1.6601e-01,  3.5211e-01, -1.6491e-01,\n",
      "         1.6566e-01,  5.3188e-02, -6.1247e-02, -2.7684e-01, -1.3897e-01,\n",
      "        -4.1943e-02,  1.6786e-01, -1.0616e+00, -5.6577e-01,  8.2518e-03,\n",
      "        -6.9683e-02,  3.5939e-01,  2.6846e-02,  1.2359e-02,  9.0945e-02,\n",
      "         2.3911e-01,  1.6260e-01, -4.7500e-01,  1.9743e-02, -3.4083e-01,\n",
      "        -1.3265e-01,  7.0563e-02, -2.0395e-01, -3.3850e-02,  6.0639e-01,\n",
      "         3.8678e-01,  3.5459e-02,  1.8894e-01,  3.4924e-01, -2.9659e-01,\n",
      "         2.0599e-01,  2.0277e-02,  4.4752e-01,  2.1413e-01, -1.4823e-01,\n",
      "        -5.1776e-01, -4.8164e-01,  3.0420e-01,  1.6959e-01, -1.8658e-01,\n",
      "         2.6334e-02,  2.8646e-01,  2.4666e-01,  2.4913e-01, -2.4955e-01,\n",
      "        -2.9828e+00,  1.8637e-02,  7.9586e-02, -1.3471e-01,  2.8371e-01,\n",
      "        -9.8590e-02,  4.1215e-02, -2.4581e-01,  3.3452e-02, -3.5669e-01,\n",
      "         6.0704e-02, -7.3479e-02, -2.6006e-01,  2.1211e-01,  3.0613e-01,\n",
      "         1.4158e-01,  1.0829e-01, -2.1730e-01,  1.7303e-01, -3.6059e-04,\n",
      "        -6.9801e-02,  2.1146e-02,  3.9076e-01,  2.7448e-01,  3.6430e-02,\n",
      "         1.1012e+00,  8.8062e-02, -2.0900e-02,  1.2877e-01,  1.3284e-01,\n",
      "        -3.5340e-01,  5.8155e-01,  1.5007e-02, -3.6992e-01,  2.4092e-01,\n",
      "        -4.2669e-01,  3.0761e-01, -3.7628e-01, -5.1724e-01, -8.5043e-02,\n",
      "         2.4013e-01, -1.6657e-01, -1.7687e-01,  2.7551e-01, -3.2495e-01,\n",
      "        -4.1502e-01,  1.1893e-01,  3.2487e-01,  1.2440e-01, -1.9468e-01,\n",
      "        -4.1386e-02, -1.3260e-02,  3.2748e-01,  2.8415e-01,  4.5670e-02,\n",
      "         2.0255e-01, -1.7916e-01,  7.5384e-02, -1.1509e-01, -2.2254e-01,\n",
      "        -4.4499e-02, -3.3520e-02,  1.2795e-01,  8.7555e-03, -2.7403e-01,\n",
      "        -5.7598e-04,  2.8106e-01,  9.4293e-02,  2.4500e-01, -5.6809e-02,\n",
      "         1.6392e-01, -7.4019e-01,  4.3703e-01, -5.8202e-01,  4.6721e-02,\n",
      "         6.3308e-03,  1.2398e-02, -4.2337e-01,  2.3446e-01,  4.4644e-02,\n",
      "        -9.0619e-02,  1.2651e-01,  5.6263e-01, -1.5457e-01, -1.9004e-01,\n",
      "         3.1192e-01,  9.3609e-02, -1.8081e-01,  2.6551e-02,  1.6922e-01,\n",
      "         4.7050e-02, -4.0018e-01, -2.3243e-01, -1.5288e+00, -1.7757e-01,\n",
      "        -1.0976e-01,  4.7913e-01,  4.0409e-02,  3.8366e-02,  7.5208e-02,\n",
      "        -2.0333e-01,  1.8850e-01, -1.4996e-01,  2.9456e-01,  3.3107e-01,\n",
      "        -3.6401e-01, -2.5702e-01, -5.0381e-01,  3.4216e-01, -3.3161e-01,\n",
      "        -1.5020e-01,  4.8240e-02, -1.0256e-01,  6.0339e-02,  3.6749e-01,\n",
      "        -1.0371e-01,  1.7180e-01,  6.4421e-02, -6.6760e-02, -7.9440e-02,\n",
      "        -2.9021e-01,  1.6336e-01, -1.5900e-01,  8.4213e-03, -2.9418e-01,\n",
      "         3.0730e-01, -1.1836e-01, -1.8304e-01, -2.1189e+00, -2.2052e-02,\n",
      "         4.1479e-02, -2.7531e-01,  7.4707e-02, -2.6809e-01, -3.2133e-02,\n",
      "         3.4080e-02, -4.4727e-01, -1.9948e-01,  1.5671e-02, -4.1699e-01,\n",
      "         3.8232e-01,  4.8622e-01,  8.5331e-02,  8.1739e-02, -6.2983e-02,\n",
      "        -3.0021e-02, -3.0823e-01,  5.5361e-02, -2.3657e-02, -2.4541e-01,\n",
      "         3.0696e-01, -3.2992e-01,  3.1408e-01, -1.0459e-01, -4.8040e-01,\n",
      "         2.4764e-01, -5.0482e-01, -6.4986e-02, -1.5067e-01, -1.6934e-01,\n",
      "        -1.9946e-01,  1.4836e-01, -6.8715e-02,  1.4590e-01, -2.6175e-01,\n",
      "         2.2873e-01,  6.7777e-01, -8.8053e-02,  2.7933e-01,  2.8178e-01,\n",
      "        -1.7584e-01, -3.7901e-02, -1.4682e-01, -4.4677e-02,  2.1640e-02,\n",
      "        -2.8961e-01,  1.2141e-02,  4.0517e-01,  3.5036e-01, -2.6509e-02,\n",
      "         3.9972e-01,  6.9234e-02,  4.2287e-02, -7.6242e-02,  2.1601e-01,\n",
      "         2.8946e-01, -1.8738e-01, -8.7229e-02,  9.3419e-02, -4.4964e-01,\n",
      "         1.2797e-01,  2.3426e-01, -5.7076e-01, -2.0748e-01, -8.0175e-02,\n",
      "        -1.0950e-01, -2.0536e-01,  3.4617e-02, -2.1756e-02,  1.0688e-01,\n",
      "         7.0351e-02, -7.0184e-01, -4.2693e-01,  1.8392e-01, -2.6863e-01,\n",
      "        -1.3145e-02,  2.1161e-01, -2.3393e-01,  3.0367e-01, -1.3370e-01,\n",
      "         9.7123e-02,  2.2505e-01,  2.6076e-02,  2.7468e-01,  3.1666e-01,\n",
      "        -1.7806e-01, -2.3418e-01,  1.1226e-01,  2.4074e-01,  1.0326e-01,\n",
      "         4.5464e-01,  6.9882e-02,  1.3645e-01,  1.2587e-01,  4.2868e-01,\n",
      "        -4.9835e-01,  2.2182e-01, -8.1946e-03, -4.7159e-01, -3.5994e-01,\n",
      "         1.2997e-01, -2.5230e-01,  2.4814e-02, -2.2406e-01, -4.2009e-01,\n",
      "         6.2146e-02, -6.2727e-01,  2.3362e-02,  2.2099e-01, -7.3865e-02,\n",
      "         3.1611e-01, -4.0114e-02,  2.7860e-01, -2.1248e-01, -2.9443e-01,\n",
      "         5.3915e-01, -2.0135e-02,  4.7865e-01,  2.1897e-02,  2.3801e-02,\n",
      "         4.5627e-03,  2.0574e-02, -2.0368e-02,  1.8676e-03, -1.8980e-01,\n",
      "        -5.3134e-01, -1.1349e-01, -2.4332e-01,  2.2042e-01,  2.8167e-01,\n",
      "        -2.5279e-01, -5.6176e-01, -3.3396e-01,  2.0545e-01,  1.7160e-01,\n",
      "         4.2664e-01,  2.0360e-01,  3.5706e-01,  3.4005e-01,  9.5801e-02,\n",
      "        -3.7548e-01, -1.6446e-01,  9.6198e-02,  4.2134e-01, -7.0829e-02,\n",
      "         2.9923e-01,  3.6841e-01,  3.0608e-01, -8.1262e-02, -2.2909e-01,\n",
      "         2.2241e-01, -2.6703e-01, -1.8286e-01, -3.5590e-01,  1.6642e-01,\n",
      "        -6.3871e-02, -2.2876e-01,  3.3502e-01, -1.9290e-01,  3.0484e-01,\n",
      "        -9.7800e-02, -2.0125e-02, -5.8609e-02, -2.9349e-01,  1.1546e-01,\n",
      "         5.5623e-02, -5.0647e-01,  3.3818e-01,  6.5793e-01, -3.5944e-01,\n",
      "         2.4696e-01, -2.3012e-01, -8.7893e-02, -5.1332e-02, -6.2112e-02,\n",
      "        -2.1150e-02,  2.6885e-01,  1.5857e-01, -1.7151e-01, -2.2841e-01,\n",
      "        -2.8474e-01,  4.6748e-02,  1.8303e-02, -1.4874e-02, -4.1969e-01,\n",
      "        -1.5379e-01,  3.8981e-01,  2.9126e-01,  3.6091e-01, -4.5579e-01,\n",
      "         3.3728e-02, -1.2431e-01, -2.3924e-01,  1.0286e-01,  2.5453e-01,\n",
      "         2.5318e-01,  3.3204e-01,  1.7743e-01,  4.8042e-01, -6.3621e-02,\n",
      "        -2.7659e-01,  1.8346e-01,  3.2006e-01,  1.7059e-01,  2.1373e-01,\n",
      "         8.6165e-02, -3.2580e-01,  4.3777e-02,  4.9909e-02, -1.9865e-02,\n",
      "        -4.1298e-01, -2.7608e-01, -1.2447e-01, -1.1670e-01,  2.0830e-01,\n",
      "         4.9257e-02,  2.2278e-01, -2.3576e-02, -2.1278e-01, -1.6970e-01,\n",
      "         1.3229e-01, -9.6146e-02, -2.3932e-01, -1.8674e-01, -6.2379e-01,\n",
      "        -3.0485e-01, -2.8906e-01, -9.2479e-02,  3.3787e-01,  3.1378e-01,\n",
      "         3.1432e-02,  1.9645e-01,  8.2739e-01,  1.0939e-01, -4.7953e-01,\n",
      "         9.9941e-04, -1.3930e-01,  2.3501e-01, -5.4223e-02, -3.3538e-01,\n",
      "        -3.7899e-01, -2.8253e-01,  9.6445e-02,  9.0743e-02,  1.4333e-01,\n",
      "         4.5851e-01,  2.7817e-01,  2.8568e-02, -2.8430e-01, -1.4282e-02,\n",
      "         2.6331e-02, -2.2680e-01,  7.8865e-03, -6.7198e-01, -1.4746e-02,\n",
      "         9.8834e-02,  1.8818e-01, -1.8008e-01, -7.0536e-02,  6.9000e-02,\n",
      "         1.0855e-01,  1.7765e-01,  1.0291e-01,  2.4068e-02, -8.1921e-02,\n",
      "         1.8450e-01,  3.0163e-01,  2.1069e-01, -1.1912e-01,  1.5641e-01,\n",
      "        -6.4629e-02, -2.3216e-02, -2.4035e-01, -8.1413e-02,  2.9264e-01,\n",
      "        -4.2218e-01, -2.9008e-01, -4.2117e-02,  8.2152e-01,  3.5905e-01,\n",
      "         2.2529e-01, -4.1409e-02,  3.0673e-01,  4.1653e-02, -1.5898e-01,\n",
      "         5.1395e-02, -4.8029e-01, -4.3888e-01,  4.5998e-01, -7.8808e-02,\n",
      "         3.5914e-02,  3.2192e-02, -2.5901e-01,  5.1300e-01, -2.7635e-01,\n",
      "         1.1466e-01, -1.9422e-01,  2.2776e-01, -2.6843e-01,  4.7961e-01,\n",
      "        -2.7470e-02, -2.4251e-01,  5.1730e-03,  1.7494e-02,  4.2066e-01,\n",
      "        -7.7847e-02, -1.1375e-01, -2.5034e-01, -2.8444e-01, -3.9607e-02,\n",
      "         1.1383e-01,  3.8838e-01, -6.6549e-01, -1.0474e-01,  2.4181e-01,\n",
      "        -4.8543e-01,  1.2350e-01,  3.2380e-01,  2.4327e-03, -2.6874e-01,\n",
      "         4.0043e-01,  6.4933e-02,  1.4837e-01,  2.4842e-01, -4.8750e-01,\n",
      "        -9.1876e-02, -1.3381e-01,  1.1047e-01,  1.7160e-01, -1.9074e-01,\n",
      "         1.0181e-01,  2.1341e-01,  3.8451e-02, -2.6019e-01,  2.5690e-01,\n",
      "        -1.3800e-01, -1.2774e-02, -2.2485e-01,  1.2252e-01,  1.2929e-01,\n",
      "        -1.7542e-01,  1.7122e-02,  1.1657e-01,  6.4461e-02, -2.2819e-01,\n",
      "        -1.7732e-01,  2.0267e-01,  1.4244e-01,  2.6507e-02, -2.1827e-01,\n",
      "        -1.4210e-01,  7.3243e-02, -3.6137e-01,  2.8335e-02,  4.0095e-01,\n",
      "         7.3894e-02,  4.1258e-02, -1.1492e+00,  8.3906e-02, -1.3042e-01,\n",
      "        -1.3256e-01, -1.7488e-01,  1.7721e-01,  3.1477e-01, -1.0153e-01,\n",
      "         4.9221e-02, -3.2997e-01,  1.7200e-01,  1.6073e-01,  9.4896e-02,\n",
      "         4.8404e-02,  3.1228e-02, -9.2348e-02,  3.1201e-01, -2.9146e-01,\n",
      "         7.0226e-02, -3.7576e-01, -1.3834e-01,  4.6188e-01, -3.0428e-01,\n",
      "         6.2363e-03, -3.1795e-01,  2.8214e-01, -3.3173e-01,  1.0812e-02,\n",
      "        -6.3048e-02,  2.4070e-01,  8.9743e-02,  2.1033e-01, -2.2087e-01,\n",
      "         2.1522e-02, -7.9617e-02,  3.5890e-02,  1.0230e-01,  2.8778e-01,\n",
      "         1.4219e-02,  2.4177e-01,  2.2641e-01,  5.7936e-01,  2.7618e-01,\n",
      "        -1.6663e-01,  2.8651e-02, -3.6280e-03,  6.5140e-02,  3.3149e-01,\n",
      "        -4.7789e-02,  1.8083e-02, -1.8785e-01, -2.3856e-01,  1.6178e-01,\n",
      "        -1.0403e+00,  3.7883e-01, -1.7641e-01, -1.7767e-01, -2.9743e-01,\n",
      "         2.2319e-01, -4.1922e-03, -9.4493e-02,  1.5888e-01, -1.3295e-01,\n",
      "         2.2147e-01,  1.4420e-01, -2.7583e-01, -2.5686e-01, -3.8405e-01,\n",
      "         6.1044e-02,  2.7207e-01,  8.0938e-03,  1.2311e-01,  1.0124e-01,\n",
      "        -8.8116e-02,  1.7907e-01, -1.2136e-01,  1.9652e-01, -8.1016e-02,\n",
      "         8.4584e-02,  2.1695e-01, -2.6907e-01, -3.2161e-01, -4.4393e-02,\n",
      "         4.0930e-01,  1.4886e-01, -2.9700e+00, -4.1152e-01,  7.5959e-02,\n",
      "         3.5740e-02,  3.9471e-01,  7.9370e-02,  3.6538e-01, -2.2272e-01,\n",
      "         1.3304e-01, -1.2118e-01,  2.5425e-01, -4.5623e-01,  2.3802e-02,\n",
      "        -9.9527e-02, -1.3974e-01,  1.0106e-01])\n",
      "tensor([1.])\n",
      "tensor([0])\n",
      "tensor([], dtype=torch.int64)\n",
      "[]\n",
      "Synonyms for money: []\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# load a pre-trained BERT model and tokenizer\n",
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "# function to vectorize words and find synonyms\n",
    "def find_synonyms(word):\n",
    "    # tokenize the input word\n",
    "    token = tokenizer.tokenize(word)\n",
    "    \n",
    "    # convert tokenized input to token ids\n",
    "    input_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    # convert token ids to pytorch tensor and wrap in a list\n",
    "    input_id = torch.tensor([input_id])\n",
    "    print('input id:', input_id)\n",
    "    \n",
    "    # generate word embeddings\n",
    "    # wrap the code block in a torch.no_grad() context to disable gradient calculation for faster computation\n",
    "    with torch.no_grad():\n",
    "        # pass the input_ids tensor to the BERT model and get the output\n",
    "        output = model(input_id)\n",
    "        print('outputs:', output)\n",
    "        # compute the mean of the last hidden state for each token and squeeze the tensor to remove the extra dimension\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "        print('embeddings:', embedding)\n",
    "    \n",
    "    # find synonyms using cosine similarity\n",
    "    # calculate the cosine similarity between embeddings of the input word and all other words\n",
    "    similarities = torch.nn.functional.cosine_similarity(embedding.unsqueeze(0), embedding.unsqueeze(0))\n",
    "    print(similarities)\n",
    "    # sort the similarities in descending order and get the indices of the sorted similarities\n",
    "    sorted_indices = torch.argsort(similarities, descending=True)\n",
    "    print(sorted_indices)\n",
    "    # top 10 indices of the sorted similarities excluding the input word itself\n",
    "    top_indices = sorted_indices[1:11]\n",
    "    print(top_indices)\n",
    "    # convert the indices to tokens and get the synonyms\n",
    "    synonyms = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in top_indices]\n",
    "    print(synonyms)\n",
    "    \n",
    "    return synonyms\n",
    "\n",
    "# word to find the synonyms for\n",
    "word = 'money'\n",
    "synonyms = find_synonyms(word)\n",
    "print(f'Synonyms for {word}: {synonyms}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2c6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for 'money': []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d64775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#define the word you want similar words for and the text string\n",
    "synonym_word = \"building\"\n",
    "textstr = \"Similar words to \" + synonym_word + \" are: [MASK].\"\n",
    "\n",
    "#create a pipeline for masked language modeling using BERT\n",
    "model = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "#get synonym predictions for the masked text\n",
    "synonym_prediction = model(textstr, top_k=10)\n",
    "\n",
    "#initialize a list to store the filtered words\n",
    "filtered_words_list = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
