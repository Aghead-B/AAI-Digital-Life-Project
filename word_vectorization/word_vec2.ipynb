{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c350fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id: tensor([[3407]])\n",
      "outputs: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.8099, -0.0580,  0.2824,  ...,  0.1177, -0.1399,  0.5079]]]), pooler_output=tensor([[ 0.1006,  0.1972, -0.3871,  ..., -0.0026,  0.1813, -0.2090]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "embeddings: tensor([-1.5501e-01,  9.7101e-02, -2.0039e-02, -2.7376e-01, -2.3086e-01,\n",
      "        -2.1555e-01,  2.6285e-01,  1.0263e-01, -1.4888e-01, -2.3009e-01,\n",
      "        -2.3984e-01, -1.4952e-01,  1.1697e-01,  6.5679e-02, -3.2519e-01,\n",
      "        -3.1836e-01, -1.0093e-01, -2.4247e-02,  1.9320e-02,  3.0708e-01,\n",
      "        -7.2634e-02, -6.8835e-02,  9.6553e-02,  1.6382e-01,  1.1619e-01,\n",
      "        -4.0722e-02, -1.9064e-01,  1.0297e-01, -1.9433e-01, -2.1519e-01,\n",
      "        -2.0154e-01, -5.6326e-02,  8.4450e-02,  1.2890e-01, -2.6629e-01,\n",
      "        -7.3395e-02,  6.7195e-02, -5.9494e-02, -4.9576e-01,  2.6328e-01,\n",
      "        -1.9147e-01, -2.6232e-01, -7.8828e-03,  6.6626e-02, -9.6798e-04,\n",
      "        -4.5802e-01, -4.3156e-01, -1.7457e-01, -2.7279e-01,  2.0672e-02,\n",
      "        -1.7508e-01,  2.0308e-01, -1.4586e-01,  3.5053e-01,  4.0315e-01,\n",
      "         1.1301e-01,  4.9048e-02, -2.9756e-01, -1.4190e-01,  1.4104e-01,\n",
      "        -1.7888e-01, -4.0581e-02, -1.9473e-02, -1.6981e-02,  1.6032e-01,\n",
      "         4.4613e-01,  2.1091e-01, -6.2682e-03, -5.6969e-01,  3.9745e-01,\n",
      "        -1.7873e-01, -3.9631e-01,  1.8359e-01,  4.9075e-02,  1.8635e-01,\n",
      "        -3.3410e-02, -2.4546e-02,  5.0812e-01, -1.2802e-01,  8.5255e-02,\n",
      "         5.1102e-02, -4.6317e-02,  1.3979e-01,  1.5559e-01,  1.6518e-01,\n",
      "         1.8900e-01, -5.9686e-02,  2.3248e-03, -2.5980e-02, -4.8142e-03,\n",
      "         7.3147e-02,  2.3614e-02,  4.7151e-02,  2.3791e-01,  2.3113e-01,\n",
      "         3.6128e-02, -2.5759e-01,  3.4815e-01, -2.5308e-01, -1.4942e-02,\n",
      "         1.2932e-01, -2.2663e-01,  2.9528e-02,  2.5567e-01, -2.2021e-01,\n",
      "         4.9431e-02,  1.2476e-01, -5.8640e-02, -4.4732e-02, -5.1872e-01,\n",
      "         4.1257e-01, -1.2556e-01,  2.2692e-01, -5.1956e-02, -1.9274e-01,\n",
      "         1.3918e-01,  3.2878e-01, -4.0028e-01,  2.1213e-01,  1.0761e-01,\n",
      "        -4.2001e-01, -1.5945e-01, -6.9730e-02,  6.5299e-01,  1.3516e-01,\n",
      "         2.3055e-01, -1.0988e-01, -1.9047e-01,  2.8685e-01, -3.7890e-01,\n",
      "         1.2828e-02,  2.6686e-01,  3.6802e-01,  2.9467e-01,  1.4083e-01,\n",
      "         9.2142e-02,  1.3371e-01,  2.8092e-02, -4.5352e-01,  2.0428e-02,\n",
      "        -8.1285e-02,  4.3733e-01, -8.9863e-01, -1.3755e-01,  1.9635e-01,\n",
      "         3.8334e-01, -4.6993e-03, -7.7888e-02,  7.5540e-02, -9.8214e-02,\n",
      "        -2.2736e-02,  1.8234e-01, -2.1178e-01, -7.5579e-02, -2.9814e-01,\n",
      "        -2.1495e-01, -1.2790e-01, -1.5230e-01,  2.5591e-01,  4.6893e-01,\n",
      "         4.3124e-01,  2.1461e-01, -7.3863e-03,  6.1813e-01,  1.2510e-01,\n",
      "         8.0610e-02, -1.3505e-01,  3.3635e-01,  1.8289e-01,  1.7424e-01,\n",
      "        -1.5360e-01, -1.6732e-01,  3.1514e-01,  1.9602e-02, -2.1538e-01,\n",
      "         1.2276e-02,  4.8847e-01,  3.9225e-01,  1.8938e-01, -2.1633e-01,\n",
      "        -3.0733e+00, -4.3071e-02, -1.0575e-01,  4.9112e-02,  1.7102e-01,\n",
      "         1.8084e-01,  1.0554e-01, -2.2311e-01, -2.2985e-01, -1.9488e-01,\n",
      "        -1.0242e-01, -1.4806e-01, -5.9852e-01,  3.5554e-01,  4.0838e-01,\n",
      "         3.7679e-02,  1.3635e-01, -1.1670e-01, -1.1208e-01,  4.2225e-01,\n",
      "         2.7397e-01, -1.5272e-01,  8.0636e-02,  3.0198e-01,  2.9923e-02,\n",
      "         9.0187e-01,  1.1627e-01, -9.5423e-02,  1.4980e-01,  2.9088e-01,\n",
      "        -8.9294e-01,  2.9182e-01,  2.6163e-01, -1.7002e-01,  2.3895e-01,\n",
      "         7.9160e-02,  3.1731e-01, -3.7393e-01, -7.0874e-01,  2.6875e-01,\n",
      "         9.5418e-02, -1.0737e-01,  1.6610e-01,  2.2925e-01,  9.2999e-02,\n",
      "        -3.7181e-01,  2.7453e-01,  1.7963e-01,  2.6292e-01,  1.6322e-01,\n",
      "         1.9750e-01, -5.5665e-04,  2.3947e-01,  1.1230e-01, -2.9564e-01,\n",
      "         3.3175e-02, -1.1745e-01, -4.1206e-01, -6.8911e-02, -2.8818e-01,\n",
      "        -8.9205e-02,  7.3038e-02,  3.5326e-01,  1.6556e-01, -4.3405e-01,\n",
      "        -1.8983e-01,  1.0028e-01,  1.7047e-01,  3.1066e-01, -8.7021e-02,\n",
      "        -8.9961e-03, -5.1833e-01,  2.2356e-01, -4.1334e-01, -1.1329e-01,\n",
      "         2.0313e-02, -2.9098e-01, -1.0144e-02,  1.3169e-01, -1.6609e-01,\n",
      "        -1.2592e-02,  3.4010e-01,  5.6176e-01, -1.4032e-01, -4.0889e-01,\n",
      "         1.5931e-01,  3.2747e-01, -1.2494e-01, -4.0519e-02, -1.2218e-01,\n",
      "         1.9955e-01,  3.0941e-02,  1.1598e-02, -9.4076e-01, -2.5765e-02,\n",
      "        -2.7215e-01,  2.9063e-01,  2.4131e-01,  2.6244e-01, -6.5047e-02,\n",
      "         3.7382e-02,  3.7328e-01, -4.7068e-01, -1.8688e-02,  3.9483e-01,\n",
      "        -2.5818e-01, -3.1247e-01, -7.6138e-01,  2.2536e-02, -2.3108e-01,\n",
      "        -2.4921e-01, -1.8266e-02,  1.8973e-01,  1.7380e-01,  4.6498e-01,\n",
      "        -3.3150e-02,  5.0149e-01,  3.5483e-01, -8.5880e-02,  6.3945e-02,\n",
      "         1.3735e-01,  9.8586e-02, -1.5595e-01, -9.8479e-02, -3.4078e-01,\n",
      "         3.3211e-01, -1.1960e-01, -4.0151e-01, -1.9610e+00, -1.6077e-01,\n",
      "         3.6167e-01, -6.7544e-01,  2.3959e-01,  1.9668e-02,  3.1314e-01,\n",
      "         4.9342e-02, -2.1578e-01, -1.5325e-04,  1.6135e-01, -1.8803e-01,\n",
      "         1.9085e-01,  1.5270e-01, -1.2181e-01,  8.4086e-02,  1.0977e-02,\n",
      "        -2.4323e-01, -3.7579e-01,  3.9473e-03, -1.4395e-01,  7.1416e-02,\n",
      "         2.8727e-01, -1.4085e-01,  4.1384e-01,  6.2402e-01,  4.3692e-01,\n",
      "        -2.4759e-01,  8.6922e-02,  1.6283e-01, -3.5960e-01, -4.5170e-01,\n",
      "        -1.6830e-01,  6.2663e-02,  8.9200e-02,  3.2893e-02,  2.2907e-01,\n",
      "         1.6035e-01,  5.5119e-01,  1.1259e-01,  7.5380e-02,  2.5889e-01,\n",
      "         1.4553e-01, -4.0406e-02,  3.5945e-02, -1.5352e-01, -1.0932e-02,\n",
      "        -5.4673e-01, -3.1700e-01,  1.0472e-01, -1.5807e-02,  1.8061e-01,\n",
      "         1.2129e-01, -1.8697e-01,  3.5313e-02, -1.7884e-01,  4.0080e-02,\n",
      "         1.1595e-01, -1.5544e-01, -3.6996e-01,  1.2476e-01, -1.3782e-01,\n",
      "         8.0981e-02,  4.7115e-02, -4.9897e-01, -1.3532e-01, -4.6634e-01,\n",
      "        -2.6544e-01, -2.8132e-01,  4.8105e-02, -8.2926e-02, -2.8814e-01,\n",
      "        -6.2775e-02, -9.2163e-01, -6.1607e-01,  1.8097e-01, -4.4128e-01,\n",
      "        -2.0471e-01,  5.8748e-01, -3.8446e-01, -1.0982e-01, -3.2547e-02,\n",
      "         2.2867e-01,  5.7252e-02, -3.5666e-02, -2.0246e-01, -3.7373e-02,\n",
      "        -5.8886e-01,  5.5283e-02, -7.0100e-02, -7.0737e-03,  1.7636e-01,\n",
      "         5.1310e-02,  1.9305e-01, -1.9121e-02,  3.9847e-01,  4.3293e-01,\n",
      "        -7.1715e-01,  2.5672e-01, -1.8591e-01, -1.1334e-01,  2.5005e-02,\n",
      "         8.2362e-02,  5.7022e-02,  5.8798e-01,  2.0574e-01, -8.0313e-01,\n",
      "         7.3317e-03, -2.6135e-01, -1.0594e-01,  1.5655e-01, -4.3650e-01,\n",
      "         3.1970e-02,  1.1717e-01,  4.0201e-03, -2.7114e-01, -1.2153e-01,\n",
      "         5.3031e-01, -1.5377e-01,  1.5262e-01,  5.2693e-02,  6.1719e-01,\n",
      "         3.0214e-02,  1.5326e-01, -2.5296e-01, -2.4727e-01, -3.0550e-01,\n",
      "        -2.8412e-01, -3.2210e-01, -1.1832e-01,  2.6244e-01,  1.9729e-01,\n",
      "        -1.8432e-01, -2.4838e-01, -4.8089e-02,  3.3787e-01,  1.9313e-01,\n",
      "         4.9455e-02,  3.6493e-02,  4.7786e-01,  9.5052e-02, -3.6210e-02,\n",
      "        -5.3903e-01, -8.7322e-02,  8.9159e-02,  2.7435e-01, -5.5548e-01,\n",
      "        -2.3323e-01,  3.8145e-01,  1.5602e-01, -2.5929e-02, -4.0863e-01,\n",
      "         4.4207e-01, -3.8843e-01, -2.2923e-01, -2.9802e-01,  4.1501e-01,\n",
      "        -3.4143e-02, -6.1450e-01,  1.5812e-01,  1.4646e-01,  1.2673e-01,\n",
      "        -6.5423e-02,  3.5701e-01, -1.2294e-01,  9.0843e-02,  1.2483e-01,\n",
      "         2.3746e-01, -4.2602e-01,  1.0208e-01,  7.4852e-02, -7.8739e-02,\n",
      "         4.0161e-01,  4.8367e-02, -5.7628e-02,  3.8669e-01,  5.4776e-02,\n",
      "        -1.4423e-01, -3.1850e-02,  4.2715e-02,  3.3172e-01, -3.2272e-01,\n",
      "        -1.0625e-01, -1.9131e-02,  5.3944e-01,  8.8388e-02,  3.7831e-02,\n",
      "        -1.5444e-01,  3.4052e-01, -9.2679e-02, -1.6868e-03,  1.0299e-01,\n",
      "        -2.9590e-01, -4.8686e-01, -1.4864e-01, -1.2022e-01,  2.3445e-01,\n",
      "        -4.1815e-01,  3.1222e-01,  3.9043e-01,  1.6067e-01, -2.8588e-01,\n",
      "        -2.7488e-01,  5.0556e-02, -3.4723e-01, -2.2368e-01,  2.5139e-01,\n",
      "        -1.3548e-01, -2.2145e-01, -6.1385e-02,  2.5575e-01, -3.4581e-01,\n",
      "        -2.0533e-01, -2.7385e-01, -9.6376e-03,  1.3513e-01,  6.0819e-02,\n",
      "        -1.5933e-01,  5.1898e-01, -1.2704e-01, -5.9643e-01, -1.4699e-02,\n",
      "         6.5102e-02,  2.0070e-01, -2.4820e-01, -4.0681e-01, -3.8486e-01,\n",
      "         9.8216e-02, -2.1731e-01, -2.4714e-01,  1.3179e-02, -1.4886e-01,\n",
      "        -1.4253e-01, -2.0785e-02,  2.8714e-01,  2.2562e-01, -2.9726e-01,\n",
      "        -4.5822e-02,  4.2501e-03,  2.7843e-01, -1.9450e-01, -2.2381e-01,\n",
      "        -5.2810e-02, -2.5606e-01, -3.5598e-02,  1.6459e-01, -8.0151e-02,\n",
      "         3.3978e-01,  1.9268e-01, -6.6006e-02,  4.6458e-03,  6.3503e-02,\n",
      "         1.5957e-01,  1.2961e-01, -1.2515e-01, -4.8301e-01, -3.1235e-01,\n",
      "         1.1374e-01,  1.9190e-02, -1.8962e-01, -1.3318e-01,  6.8678e-02,\n",
      "         3.4787e-01,  2.2740e-01,  2.5339e-01,  2.2589e-01,  1.5343e-01,\n",
      "         2.0143e-01,  1.7433e-01,  5.3675e-01, -6.2844e-01,  1.9366e-01,\n",
      "        -6.2370e-02, -4.3485e-02, -5.3834e-01, -1.1336e-01,  3.1057e-01,\n",
      "        -4.0940e-01, -2.1420e-01,  9.8848e-02,  8.0377e-01,  5.6819e-01,\n",
      "         7.7287e-02, -1.8748e-01,  2.0689e-01, -1.4188e-01, -1.3094e-01,\n",
      "         2.2485e-02, -2.4385e-01, -2.0217e-01,  1.4475e-01,  3.2892e-02,\n",
      "        -8.8500e-02,  3.4091e-01,  1.3411e-01,  5.3490e-01, -3.1278e-01,\n",
      "         3.7816e-01, -6.5767e-01,  1.8613e-01,  5.0648e-02,  4.5729e-01,\n",
      "        -2.4406e-01, -6.5210e-02, -1.8899e-01,  2.9627e-01,  1.9259e-01,\n",
      "        -3.9005e-01, -8.8478e-02, -1.3808e-01, -3.6004e-01,  1.3668e-01,\n",
      "         2.7092e-01,  5.0752e-01, -4.9004e-01, -2.9148e-02, -7.2816e-03,\n",
      "        -6.7251e-01, -1.1718e-01,  5.0431e-01,  7.9687e-02, -2.8038e-02,\n",
      "         2.2447e-01,  3.6661e-01,  3.4867e-01,  4.0656e-01, -6.5674e-01,\n",
      "        -2.7173e-01, -2.6698e-01,  4.6564e-01,  9.6750e-02,  6.9250e-02,\n",
      "        -5.3170e-02,  4.0142e-01,  1.0715e-01, -2.6000e-01,  6.2293e-01,\n",
      "         1.6140e-01, -2.1952e-01, -7.5004e-02,  1.0735e-01,  2.4144e-01,\n",
      "        -1.4418e-01, -1.0482e-02,  1.2108e-01,  9.6470e-02, -3.5808e-02,\n",
      "        -3.1157e-02,  3.8779e-01, -4.1062e-02,  3.4035e-01, -2.1456e-02,\n",
      "        -1.4343e-01, -2.2724e-02, -1.6527e-01, -4.6426e-02,  2.3473e-01,\n",
      "         7.2477e-02, -1.0432e-02, -9.9707e-01, -9.0783e-02, -2.5569e-01,\n",
      "        -3.2513e-01,  7.1355e-02,  7.1361e-02,  5.3581e-01,  2.2328e-01,\n",
      "        -8.5172e-02, -2.7977e-01,  8.6608e-02, -4.8835e-03,  3.1044e-01,\n",
      "         1.6645e-01,  3.6748e-01, -1.6587e-01,  4.2296e-01, -2.8936e-01,\n",
      "        -1.7412e-01, -6.5771e-01, -2.5683e-01,  2.7754e-01, -2.7531e-02,\n",
      "        -1.3734e-01, -3.0121e-01,  2.3536e-02, -2.3380e-01, -4.8816e-02,\n",
      "        -2.5062e-01,  2.0841e-01, -2.0067e-01,  4.8815e-01, -3.7007e-01,\n",
      "        -7.8328e-02, -7.5890e-02,  2.2440e-01, -2.4978e-01,  3.7611e-01,\n",
      "         1.6343e-01,  3.7932e-01,  2.4088e-01,  7.1603e-01, -1.2290e-01,\n",
      "        -8.8822e-02,  3.6252e-01, -1.8804e-01,  2.2300e-01,  2.0106e-01,\n",
      "        -3.1012e-02,  1.4135e-01, -4.1838e-01, -6.3628e-02,  4.6927e-01,\n",
      "        -1.1553e+00,  1.5977e-01,  1.4921e-01, -3.2925e-01, -8.2035e-02,\n",
      "         2.2745e-01, -5.3816e-02, -1.0957e-01,  1.9586e-02, -3.0669e-01,\n",
      "         3.3397e-01,  7.0607e-02, -1.1834e-01, -1.9855e-01, -2.1515e-01,\n",
      "         2.2996e-01,  1.8068e-01, -1.3457e-01,  2.1148e-02,  4.2849e-01,\n",
      "         3.9103e-01, -4.1020e-01,  1.4973e-01, -2.7368e-02,  1.0981e-01,\n",
      "        -2.8900e-01,  2.2329e-01, -2.9030e-01,  1.3645e-02,  8.1796e-02,\n",
      "        -2.1530e-01,  8.4036e-02, -2.8799e+00, -4.7496e-02,  2.9857e-01,\n",
      "        -2.1574e-01,  3.8028e-02, -4.5146e-01,  3.0865e-01, -2.8204e-01,\n",
      "         2.4993e-01, -2.4210e-01,  1.9425e-01, -1.4150e-01,  7.2935e-02,\n",
      "         1.6464e-02, -2.1696e-01, -9.9669e-02])\n",
      "tensor([1.0000])\n",
      "tensor([0])\n",
      "tensor([], dtype=torch.int64)\n",
      "[]\n",
      "Synonyms for Happy: []\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# load a pre-trained BERT model and tokenizer\n",
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "# function to vectorize words and find synonyms\n",
    "def find_synonyms(word):\n",
    "    # tokenize the input word\n",
    "    token = tokenizer.tokenize(word)\n",
    "    \n",
    "    # convert tokenized input to token ids\n",
    "    input_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    # convert token ids to pytorch tensor and wrap in a list\n",
    "    input_id = torch.tensor([input_id])\n",
    "    print('input id:', input_id)\n",
    "    \n",
    "    # generate word embeddings\n",
    "    # wrap the code block in a torch.no_grad() context to disable gradient calculation for faster computation\n",
    "    with torch.no_grad():\n",
    "        # pass the input_ids tensor to the BERT model and get the output\n",
    "        output = model(input_id)\n",
    "        print('outputs:', output)\n",
    "        # compute the mean of the last hidden state for each token and squeeze the tensor to remove the extra dimension\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "        print('embeddings:', embedding)\n",
    "    \n",
    "    # find synonyms using cosine similarity\n",
    "    # calculate the cosine similarity between embeddings of the input word and all other words\n",
    "    similarities = torch.nn.functional.cosine_similarity(embedding.unsqueeze(0), embedding.unsqueeze(0))\n",
    "    print(similarities)\n",
    "    # sort the similarities in descending order and get the indices of the sorted similarities\n",
    "    sorted_indices = torch.argsort(similarities, descending=True)\n",
    "    print(sorted_indices)\n",
    "    # top 10 indices of the sorted similarities excluding the input word itself\n",
    "    top_indices = sorted_indices[1:11]\n",
    "    print(top_indices)\n",
    "    # convert the indices to tokens and get the synonyms\n",
    "    synonyms = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in top_indices]\n",
    "    print(synonyms)\n",
    "    \n",
    "    return synonyms\n",
    "\n",
    "# word to find the synonyms for\n",
    "word = 'Happy'\n",
    "synonyms = find_synonyms(word)\n",
    "print(f'Synonyms for {word}: {synonyms}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b0c486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building', 'construction', 'tower', 'architecture', 'structure', 'house', 'office', 'housing', 'hall']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#define the word you want similar words for and the text string\n",
    "synonym_word = \"building\"\n",
    "textstr = \"Similar words to \" + synonym_word + \" are: [MASK].\"\n",
    "\n",
    "#create a pipeline for masked language modeling using BERT\n",
    "model = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "#get synonym predictions for the masked text\n",
    "synonym_prediction = model(textstr, top_k=10)\n",
    "\n",
    "#initialize a list to store the filtered words\n",
    "filtered_words_list = []\n",
    "\n",
    "#iterate through the synonym predictions\n",
    "for x in synonym_prediction:\n",
    "    token_str = x['token_str']\n",
    "\n",
    "#filter out non-alphabetic tokens\n",
    "    if token_str.isalpha():\n",
    "        filtered_words_list.append(token_str)\n",
    "\n",
    "#print the filtered words\n",
    "print(filtered_words_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f9f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
